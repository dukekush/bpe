{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import imageio as io\n",
    "\n",
    "import argparse\n",
    "\n",
    "from bpe.functional.motion import preprocess_motion2d_rc, cocopose2motion, json2annotations, annotations2motion\n",
    "\n",
    "from bpe import Config\n",
    "\n",
    "from bpe.similarity_analyzer import SimilarityAnalyzer\n",
    "\n",
    "from bpe.functional.motion import preprocess_motion2d_rc, cocopose2motion\n",
    "\n",
    "from bpe.functional.utils import pad_to_height\n",
    "\n",
    "from bpe.functional.visualization import preprocess_sequence, video_out_with_imageio\n",
    "\n",
    "data_dir = 'bpe-datasets/SARA_released'\n",
    "model_path = 'train_log/exp_bpe/model/model_best.pth.tar'\n",
    "video1 = ''\n",
    "video2 = ''\n",
    "# v1 = 'bpe-datasets/refined_skeleton/007/S001C001P004R001A007.json'\n",
    "# v2 = 'bpe-datasets/refined_skeleton/007/S001C001P005R002A007.json'\n",
    "v1 = 'attack_pose/json_files/GX010032_Clip_10_sec_24_POSE_tabel.json'\n",
    "v2 = 'attack_pose/json_files/GX010033_Clip_6_sec_14_POSE_tabel.json'\n",
    "img1_height = 1080\n",
    "img1_width = 1920\n",
    "img2_height = 1080\n",
    "img2_width = 1920\n",
    "pad2 = 0\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--name', type=str, default=\"sim_test\", help=\"task name\")\n",
    "parser.add_argument('--data_dir', default=\"\", required=True, help=\"path to dataset dir\")\n",
    "parser.add_argument('--model_path', type=str, required=True, help=\"filepath for trained model weights\")\n",
    "parser.add_argument('--video1', type=str, required=True, help=\"video1 mp4 path\", default=None)\n",
    "parser.add_argument('--video2', type=str, required=True, help=\"video2 mp4 path\", default=None)\n",
    "parser.add_argument('-v1', '--vid1_json_dir', type=str, required=True, help=\"video1's coco annotation json\")\n",
    "parser.add_argument('-v2', '--vid2_json_dir', type=str, required=True, help=\"video2's coco annotation json\")\n",
    "parser.add_argument('-h1', '--img1_height', type=int, help=\"video1's height\", default=480)\n",
    "parser.add_argument('-w1', '--img1_width', type=int, help=\"video1's width\", default=854)\n",
    "parser.add_argument('-h2', '--img2_height', type=int, help=\"video2's height\", default=480)\n",
    "parser.add_argument('-w2', '--img2_width', type=int, help=\"video2's width\", default=854)\n",
    "parser.add_argument('-pad2', '--pad2', type=int, help=\"video2's start frame padding\", default=0)\n",
    "parser.add_argument('-g', '--gpu_ids', type=int, default=0, required=False)\n",
    "parser.add_argument('--out_path', type=str, default='./visual_results', required=False)\n",
    "parser.add_argument('--out_filename', type=str, default='twice.mp4', required=False)\n",
    "parser.add_argument('--use_flipped_motion', action='store_true',\n",
    "                    help=\"whether to use one decoder per one body part\")\n",
    "parser.add_argument('--use_invisibility_aug', action='store_true',\n",
    "                    help=\"change random joints' visibility to invisible during training\")\n",
    "parser.add_argument('--debug', action='store_true', help=\"limit to 500 frames\")\n",
    "# related to video processing\n",
    "parser.add_argument('--video_sampling_window_size', type=int, default=16,\n",
    "                    help='window size to use for similarity prediction')\n",
    "parser.add_argument('--video_sampling_stride', type=int, default=16,\n",
    "                    help='stride determining when to start next window of frames')\n",
    "parser.add_argument('--use_all_joints_on_each_bp', action='store_true',\n",
    "                    help=\"using all joints on each body part as input, as opposed to particular body part\")\n",
    "\n",
    "parser.add_argument('--similarity_measurement_window_size', type=int, default=1,\n",
    "                    help='measuring similarity over # of oversampled video sequences')\n",
    "parser.add_argument('--similarity_distance_metric', choices=[\"cosine\", \"l2\"], default=\"cosine\")\n",
    "parser.add_argument('--privacy_on', action='store_true',\n",
    "                    help='when on, no original video or sound in present in the output video')\n",
    "parser.add_argument('--thresh', type=float, default=0.5, help='threshold to seprate positive and negative classes')\n",
    "parser.add_argument('--connected_joints', action='store_true', help='connect joints with lines in the output video')\n",
    "\n",
    "\n",
    "args = parser.parse_args([\n",
    "    '--data_dir', data_dir, \n",
    "    '--model_path', model_path,\n",
    "    '--video1', video1,\n",
    "    '--video2', video2,\n",
    "    '-v1', v1,\n",
    "    '-v2', v2,\n",
    "    '--img1_height', str(img1_height),\n",
    "    '--img1_width', str(img1_width),\n",
    "    '--img2_height', str(img2_height),\n",
    "    '--img2_width', str(img2_width),\n",
    "    '--pad2', str(pad2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Loading model from train_log/exp_bpe/model/model_best.pth.tar\n",
      "Model is ready\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ra': 0.9466005563735962,\n",
       "  'la': 0.45262736082077026,\n",
       "  'rl': 0.9769474267959595,\n",
       "  'll': 0.9909589290618896,\n",
       "  'torso': 0.6687353253364563},\n",
       " {'ra': 0.9531256556510925,\n",
       "  'la': 0.2960130274295807,\n",
       "  'rl': 0.9530953764915466,\n",
       "  'll': 0.9929720759391785,\n",
       "  'torso': 0.5830177068710327},\n",
       " {'ra': 0.9106404185295105,\n",
       "  'la': 0.7238216400146484,\n",
       "  'rl': 0.958930492401123,\n",
       "  'll': 0.9873378872871399,\n",
       "  'torso': 0.3882153034210205},\n",
       " {'ra': 0.708549976348877,\n",
       "  'la': 0.504763662815094,\n",
       "  'rl': 0.9712213277816772,\n",
       "  'll': 0.9660616517066956,\n",
       "  'torso': 0.8233088254928589},\n",
       " {'ra': 0.8118758201599121,\n",
       "  'la': 0.9140557050704956,\n",
       "  'rl': 0.9199668169021606,\n",
       "  'll': 0.9897566437721252,\n",
       "  'torso': 0.9306679964065552},\n",
       " {'ra': 0.9598767161369324,\n",
       "  'la': 0.6277140378952026,\n",
       "  'rl': 0.921382486820221,\n",
       "  'll': 0.9378729462623596,\n",
       "  'torso': 0.913017213344574},\n",
       " {'ra': 0.8186538219451904,\n",
       "  'la': 0.7713695764541626,\n",
       "  'rl': 0.6813949346542358,\n",
       "  'll': 0.966238260269165,\n",
       "  'torso': 0.9286928176879883},\n",
       " {'ra': 0.8026408553123474,\n",
       "  'la': 0.09088508784770966,\n",
       "  'rl': 0.7808101773262024,\n",
       "  'll': 0.965143620967865,\n",
       "  'torso': 0.7392195463180542},\n",
       " {'ra': 0.9329681396484375,\n",
       "  'la': 0.35719192028045654,\n",
       "  'rl': 0.9631553888320923,\n",
       "  'll': 0.9704767465591431,\n",
       "  'torso': 0.6013302206993103},\n",
       " {'ra': 0.892105221748352,\n",
       "  'la': 0.8648087978363037,\n",
       "  'rl': 0.9701473712921143,\n",
       "  'll': 0.9933100938796997,\n",
       "  'torso': 0.6931125521659851},\n",
       " {'ra': 0.9152448177337646,\n",
       "  'la': 0.46651551127433777,\n",
       "  'rl': 0.9374200105667114,\n",
       "  'll': 0.9699411988258362,\n",
       "  'torso': 0.6571177244186401}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pose_bpe = np.load(os.path.join(data_dir, 'meanpose_rc_with_view_unit64.npy'))\n",
    "std_pose_bpe = np.load(os.path.join(data_dir, 'stdpose_rc_with_view_unit64.npy'))\n",
    "mean_pose_bpe.shape, std_pose_bpe.shape\n",
    "\n",
    "# if not os.path.exists(args.out_path):\n",
    "#         os.makedirs(args.out_path)\n",
    "\n",
    "config = Config(args)\n",
    "similarity_analyzer = SimilarityAnalyzer(config, args.model_path)\n",
    "\n",
    "# for NTU-RGB test - it used w:1920, h:1080\n",
    "h1, w1, scale1 = pad_to_height(config.img_size[0], args.img1_height, args.img1_width)\n",
    "h2, w2, scale2 = pad_to_height(config.img_size[0], args.img2_height, args.img2_width)\n",
    "h1, w1, scale1, h2, w2, scale2\n",
    "\n",
    "# get input suitable for motion similarity analyzer\n",
    "seq1 = cocopose2motion(config.unique_nr_joints, args.vid1_json_dir, scale=scale1,\n",
    "                        visibility=args.use_invisibility_aug)\n",
    "seq2 = cocopose2motion(config.unique_nr_joints, args.vid2_json_dir, scale=scale2,\n",
    "                        visibility=args.use_invisibility_aug)[:, :, args.pad2:]\n",
    "seq1.shape, seq2.shape\n",
    "\n",
    "seq1 = preprocess_sequence(seq1)\n",
    "seq2 = preprocess_sequence(seq2)\n",
    "\n",
    "seq1_origin = preprocess_motion2d_rc(seq1, mean_pose_bpe, std_pose_bpe,\n",
    "                                        invisibility_augmentation=args.use_invisibility_aug,\n",
    "                                        use_all_joints_on_each_bp=args.use_all_joints_on_each_bp)\n",
    "seq2_origin = preprocess_motion2d_rc(seq2, mean_pose_bpe, std_pose_bpe,\n",
    "                                        invisibility_augmentation=args.use_invisibility_aug,\n",
    "                                        use_all_joints_on_each_bp=args.use_all_joints_on_each_bp)\n",
    "\n",
    "seq1.shape, seq2.shape, seq1_origin.shape, seq2_origin.shape\n",
    "\n",
    "seq1_origin = seq1_origin.to(config.device)\n",
    "seq2_origin = seq2_origin.to(config.device)\n",
    "\n",
    "# get embeddings\n",
    "seq1_features = similarity_analyzer.get_embeddings(seq1_origin, video_window_size=args.video_sampling_window_size,\n",
    "                                                       video_stride=args.video_sampling_stride)\n",
    "seq2_features = similarity_analyzer.get_embeddings(seq2_origin, video_window_size=args.video_sampling_window_size,\n",
    "                                                       video_stride=args.video_sampling_stride)\n",
    "\n",
    "# get motion similarity\n",
    "motion_similarity_per_window = \\\n",
    "    similarity_analyzer.get_similarity_score(seq1_features, seq2_features,\n",
    "                                                similarity_window_size=args.similarity_measurement_window_size)\n",
    "\n",
    "motion_similarity_per_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpe_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
